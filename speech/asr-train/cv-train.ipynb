{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune an Automatic Speech Recognition (ASR) AI model\n",
    "\n",
    "> Indented block\n",
    "\n",
    "> Indented block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "\n",
    "**Package Installations**\n",
    "\n",
    "This code runs shell commands and install relevant python packages and libraries.\n",
    "\n",
    "**GPU Configurations**\n",
    "\n",
    "This code checks for GPU availability and prints their details. If a GPU is available, it compares the execution time of a matrix multiplication operation on both the CPU and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "# %run misc.setup.py (COMMENT THIS LINE AFTER RUNNING THE CELL ONCE)\n",
    "%run misc.gpu_config.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Library Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "%pip uninstall imageio -y\n",
    "%pip install imageio\n",
    "%pip uninstall pillow -y\n",
    "%pip install --no-cache-dir pillow\n",
    "%pip install datasets huggingface-hub librosa wordcloud\n",
    "%pip install --upgrade evaluate jiwer\n",
    "%pip install seaborn\n",
    "%pip install transformers\n",
    "%pip install tf-keras\n",
    "%pip uninstall -y tensorflow tensorflow-macos tensorflow-metal keras\n",
    "%pip install tensorflow==2.16.1 tensorflow-metal==1.1.0 keras==3.0.0 \n",
    "%pip install pydub\n",
    "# install ffmpeg: pydub relies on ffprobe to process audio files like MP3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                                        # Operating system-related functions (file, directory operations)\n",
    "import sys                                       # Provides access to system-specific parameters and functions\n",
    "import platform                                  # Provides functions and information about the operating system and hardware\n",
    "\n",
    "import numpy as np                               # numerical operations & array manipulation\n",
    "import pandas as pd\n",
    "import pickle                                    # for object serialization & deserialization\n",
    "import random                                    # generates random numbers\n",
    "import math\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt                  # Creating plots & visualizations\n",
    "import seaborn as sns                            # Statistical data visualization library\n",
    "\n",
    "# import cv2                                       # computer vision & image processing\n",
    "import tensorflow as tf                          # deep learning framework\n",
    "import keras                                     # training & evaluating deep learning models\n",
    "import transformers\n",
    "from tqdm import tqdm                            # creating progress bars in loops\n",
    "from typing import Dict, List, Tuple, Optional   # type hinting\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Audio / Speech Processing\n",
    "import huggingface_hub\n",
    "import librosa\n",
    "import wordcloud\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the current directory to the Python path\n",
    "# sys.path.append(os.path.abspath('.'))\n",
    "\n",
    "from misc.print_versions import print_system_and_package_info\n",
    "\n",
    "libraries_to_check = [\n",
    "    'conda', 'pip', '',\n",
    "    # General Python Libraries\n",
    "    'numpy', 'matplotlib', 'seaborn', 'tqdm', '',\n",
    "    # Audio / Speech Processing\n",
    "    'keras', 'tensorflow', 'transformers', 'huggingface_hub', 'librosa', 'wordcloud']\n",
    "\n",
    "print_system_and_package_info(libraries_to_check, write_to_file=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "\n",
    "For this project, we will be using the [Common Voice](https://www.kaggle.com/datasets/mozillaorg/common-voice/data) dataset sourced from Kaggle. The audio clips for each subset are stored as mp3 files in folders with the same naming conventions as their corresponding CSV files. For instance, all audio data from the valid train set is stored in the folder `cv-valid-train`, alongside the `cv-valid-train.csv` metadata file.\n",
    "\n",
    "### **CSV File Structure**\n",
    "\n",
    "Each row in the CSV file represents a single audio clip and contains the following information:\n",
    "\n",
    "- **`filename`**: Relative path of the audio file.\n",
    "- **`text`**: Supposed transcription of the audio.\n",
    "- **`up_votes`**: Number of people who confirmed the audio matches the text.\n",
    "- **`down_votes`**: Number of people who reported the audio does not match the text.\n",
    "- **`age`**: Age of the speaker, if reported:\n",
    "  - `teens`: `< 19`\n",
    "  - `twenties`: `19 - 29`\n",
    "  - `thirties`: `30 - 39`\n",
    "  - `fourties`: `40 - 49`\n",
    "  - `fifties`: `50 - 59`\n",
    "  - `sixties`: `60 - 69`\n",
    "  - `seventies`: `70 - 79`\n",
    "  - `eighties`: `80 - 89`\n",
    "  - `nineties`: `> 89`\n",
    "- **`gender`**: Gender of the speaker, if reported:\n",
    "  - `male`\n",
    "  - `female`\n",
    "  - `other`\n",
    "- **`accent`**: Accent of the speaker, if reported:\n",
    "  - `us`: `United States English`\n",
    "  - `australia`: `Australian English`\n",
    "  - `england`: `England English`\n",
    "  - `canada`: `Canadian English`\n",
    "  - `philippines`: `Filipino`\n",
    "  - `hongkong`: `Hong Kong English`\n",
    "  - `indian`: `India and South Asia (India, Pakistan, Sri Lanka)`\n",
    "  - `ireland`: `Irish English`\n",
    "  - `malaysia`: `Malaysian English`\n",
    "  - `newzealand`: `New Zealand English`\n",
    "  - `scotland`: `Scottish English`\n",
    "  - `singapore`: `Singaporean English`\n",
    "  - `southatlandtic`: `South Atlantic (Falkland Islands, Saint Helena)`\n",
    "  - `african`: `Southern African (South Africa, Zimbabwe, Namibia)`\n",
    "  - `wales`: `Welsh English`\n",
    "  - `bermuda`: `West Indies and Bermuda (Bahamas, Bermuda, Jamaica, Trinidad)`\n",
    "\n",
    "### **Acknowledgments**\n",
    "This dataset was compiled by Michael Henretty, Tilman Kamp, Kelly Davis, and The Common Voice Team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo\n",
    "# Creating a grid for subplots with 2 columns\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 15), constrained_layout=True)\n",
    "\n",
    "# 1. Up Votes vs. Down Votes\n",
    "sns.scatterplot(data=df, x='up_votes', y='down_votes', alpha=0.6, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Up Votes vs. Down Votes')\n",
    "axes[0, 0].set_xlabel('Up Votes')\n",
    "axes[0, 0].set_ylabel('Down Votes', rotation=0, labelpad=40)\n",
    "\n",
    "# 2. Age Group Distribution\n",
    "age_counts = df['age'].value_counts()\n",
    "age_counts.plot(kind='bar', color='skyblue', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Age Group Distribution')\n",
    "axes[0, 1].set_xlabel('Age Group')\n",
    "axes[0, 1].set_ylabel('Count', rotation=0, labelpad=40)\n",
    "axes[0, 1].tick_params(axis='x', rotation=0)  # Ensure x-axis labels are horizontal\n",
    "\n",
    "# 3. Gender Distribution\n",
    "gender_counts = df['gender'].value_counts()\n",
    "gender_counts.plot(kind='pie', autopct='%1.1f%%', startangle=90, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Gender Distribution')\n",
    "axes[1, 0].set_ylabel('')  # Clear default ylabel\n",
    "\n",
    "# 4. Accent Distribution\n",
    "accent_counts = df['accent'].value_counts()\n",
    "accent_counts.plot(kind='bar', color='coral', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Accent Distribution')\n",
    "axes[1, 1].set_xlabel('Accent')\n",
    "axes[1, 1].set_ylabel('Count', rotation=0, labelpad=40)\n",
    "\n",
    "# 5. Percentage of Missing Values per Column\n",
    "missing_values = df.isnull().sum()\n",
    "total_rows = len(df)\n",
    "missing_percentage = (missing_values / total_rows) * 100\n",
    "missing_percentage = missing_percentage[missing_percentage > 0]  # Filter columns with missing data\n",
    "missing_percentage.sort_values().plot(kind='bar', color='orange', alpha=0.8, ax=axes[2, 1])\n",
    "axes[2, 1].set_title('Percentage of Missing Values per Column')\n",
    "axes[2, 1].set_xlabel('Column')\n",
    "axes[2, 1].set_ylabel('Percentage', rotation=0, labelpad=40)\n",
    "axes[2, 1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Pre-processing\n",
    "\n",
    "#### Data Cleaning\n",
    " * remove NaN or null values\n",
    " * remove irrelevant/ unnecessary data\n",
    "\n",
    "#### Train-Validation Split\n",
    "\n",
    "#### Preliminary Data Visualisation\n",
    "* Waveform Visualization\n",
    "* Mel-frequency cepstral coefficients (MFCCs) / Spectrogram\n",
    "\n",
    "\n",
    "#### Data Extraction (Feature Engineering)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# File paths\n",
    "csv_file = '../common_voice/cv-valid-train.csv'\n",
    "audio_dir = '../common_voice/cv-valid-train'\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Display dataset overview\n",
    "print('Dataset Overview:')\n",
    "display(HTML(df.head().to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo\n",
    "# Loading data online\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "common_voice['train'] = load_dataset('mozilla-foundation/common_voice_11_0', 'en', split='train+validation')\n",
    "# common_voice['test'] = load_dataset('mozilla-foundation/common_voice_13_0', 'en', split='test')\n",
    "# DatasetNotFoundError: Dataset 'mozilla-foundation/common_voice_13_0' is a gated dataset on the Hub. You must be authenticated to access it.\n",
    "\n",
    "# Perform a custom split (70% train, 30% validation)\n",
    "split_datasets = dataset.train_test_split(test_size=0.3, seed=0)\n",
    "train_dataset  = split_datasets['train']\n",
    "val_dataset    = split_datasets['test']\n",
    "print(f'Train size: {len(train_dataset)}, Validation size: {len(val_dataset)}')\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "from data_cleaning import preprocess_data, visualize_audio_features\n",
    "\n",
    "# Training-Validation Split: 70-30\n",
    "train_df, val_df = train_test_split(df, test_size=0.3, random_state=123)\n",
    "\n",
    "# Save the splits if needed\n",
    "train_df.to_csv('train_split.csv', index=False)\n",
    "val_df.to_csv('val_split.csv', index=False)\n",
    "\n",
    "# Display number of samples in each split\n",
    "print(f'Number of data in csv: {len(df)}')\n",
    "print(f'Number of data in training: {len(train_df)}')\n",
    "print(f'Number of data in validation: {len(val_df)}')\n",
    "\n",
    "# Randomly select x files for testing\n",
    "x = 3\n",
    "df_sample = train_df.sample(x, random_state=0).reset_index(drop=True)\n",
    "\n",
    "# Preprocess audio files\n",
    "print('\\nPreprocessing audio files...')\n",
    "processed_df = preprocess_data(df_sample, audio_dir)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print('\\nProcessed DataFrame with features:')\n",
    "display(processed_df)\n",
    "\n",
    "# Play the first processed audio file\n",
    "print('\\nPlaying audio samples:')\n",
    "for idx, row in processed_df.iterrows():\n",
    "    if 'processed_path' in row and pd.notnull(row['processed_path']):\n",
    "        print(f\"\\nPlaying {row['processed_path']}\")\n",
    "        display(Audio(filename=row['processed_path']))\n",
    "        if idx >= x:  # Limit playback to x files\n",
    "            break\n",
    "\n",
    "# Visualize features for the processed audio files\n",
    "print('\\nVisualizing audio features for the selected files...')\n",
    "visualize_audio_features(processed_df, audio_dir, sample_num=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Extraction (Feature Engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Cloud\n",
    "text_data = ' '.join(df['text'].dropna())\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text_data)\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of Audio Transcripts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run misc/evaluation_metrics.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Choice\n",
    "\n",
    "The chosen ASR AI Model is [wav2vec2-large-960h](https://huggingface.co/facebook/wav2vec2-large-960h). This model is developed by Facebook and pretrained and fine-tuned on Librispeech dataset on 16kHz sampled speech audio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use a pipeline as a high-level helper\n",
    "# from transformers import pipeline\n",
    "# pipe = pipeline('automatic-speech-recognition', model='facebook/wav2vec2-large-960h')\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForCTC\n",
    "# # Load processor and model directly using downloaded paths\n",
    "# processor = AutoProcessor.from_pretrained('facebook/wav2vec2-large-960h')\n",
    "# model = AutoModelForCTC.from_pretrained('facebook/wav2vec2-large-960h')\n",
    "# model_dir = 'model'\n",
    "# processor.save_pretrained(model_dir)\n",
    "# model.save_pretrained(model_dir)\n",
    "\n",
    "# Load processor and model from the saved directory\n",
    "model_dir = 'model'\n",
    "processor = AutoProcessor.from_pretrained(model_dir)\n",
    "model = AutoModelForCTC.from_pretrained(model_dir)\n",
    "\n",
    "# Verify that the model and processor are loaded\n",
    "print('Processor and model successfully loaded from the folder:', model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Processor\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# Feature extraction for raw audio\n",
    "audio_dir: str = '../common_voice/cv-valid-train/cv-valid-train/sample-031723.mp3'\n",
    "audio: AudioSegment = AudioSegment.from_file(audio_dir)\n",
    "\n",
    "# Convert audio to numpy array (normalized to [-1, 1])\n",
    "samples: np.ndarray = np.array(audio.get_array_of_samples(), dtype=np.float32)\n",
    "samples = samples / np.iinfo(audio.array_type).max  # Normalize to range [-1, 1]\n",
    "\n",
    "# Load Wav2Vec2Processor (handles both feature extraction and tokenization)\n",
    "processor: Wav2Vec2Processor = Wav2Vec2Processor.from_pretrained('facebook/wav2vec2-large-960h')\n",
    "\n",
    "# Extract features for the model\n",
    "input_values: torch.Tensor = processor.feature_extractor(samples, sampling_rate=16000, return_tensors='pt').input_values\n",
    "\n",
    "# Tokenizing text transcriptions correctly\n",
    "labels: torch.Tensor = processor.tokenizer('this is a sample transcription', return_tensors='pt').input_ids\n",
    "\n",
    "# Debug: Print types to trace input and output\n",
    "print(f'Audio Type: {type(audio)}')                                             # <class 'pydub.audio_segment.AudioSegment'>\n",
    "print(f'Samples Type: {type(samples)}, Shape: {samples.shape}')                 # <class 'numpy.ndarray'>\n",
    "print(f'Input Values Type: {type(input_values)}, Shape: {input_values.shape}')  # <class 'torch.Tensor'>\n",
    "print(f'Labels Type: {type(labels)}, Shape: {labels.shape}')                    # <class 'torch.Tensor'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC, Trainer, TrainingArguments\n",
    "\n",
    "# Load model\n",
    "model = Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-large-960h',\n",
    "    ctc_loss_reduction='mean',\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    ")\n",
    "# model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\", vocab_size=len(tokenizer))\n",
    "    \n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./wav2vec2-finetuned',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=500,\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_df,\n",
    "    eval_dataset=val_df,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    # tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation\n",
    "\n",
    "For evaluating models like `Wav2Vec2ForCTC` in tasks such as Automatic Speech Recognition (ASR), the following metrics are typically used:\n",
    "\n",
    "### 1. Word Error Rate (WER)\n",
    "- **Definition**: Measures the percentage of incorrectly predicted words in the transcript.\n",
    "- **Formula**:\n",
    "  $WER = \\frac{S + D + I}{N}$\n",
    "  where:\n",
    "  - \\( S \\): Number of substitutions (wrong word predicted).\n",
    "  - \\( D \\): Number of deletions (missing words in prediction).\n",
    "  - \\( I \\): Number of insertions (extra words in prediction).\n",
    "  - \\( N \\): Total number of words in the reference transcript.\n",
    "- **Usage**:\n",
    "  - Lower WER indicates better model performance.\n",
    "  - It is the most common metric for ASR systems.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Understanding (Explainability)\n",
    "\n",
    "The implementation from sklearn will serve as a benchmark for us to determine how good the current implementation of decision tree model and the k nearest neighbours classifier is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Deployment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
